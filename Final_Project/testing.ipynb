{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test script\n",
    "\n",
    "Script to run a test on recent data. The underlying assumption of the testing procedure is that if an ML model predicts that a drive will fail, then the hard drive would be taken out of operation. Therefore, at each day predictions are only made for drives for which no prediction of failure had been made prior to\n",
    "that day.\n",
    "\n",
    "The primary metrics we wish to track are the number of unexpected breaks and the number of false alarms that would have been output by the ML models had they been used during the first half of 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-11T20:39:11.501669Z",
     "iopub.status.busy": "2020-08-11T20:39:11.500671Z",
     "iopub.status.idle": "2020-08-11T20:39:13.280103Z",
     "shell.execute_reply": "2020-08-11T20:39:13.279162Z",
     "shell.execute_reply.started": "2020-08-11T20:39:11.501562Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-11T22:50:20.810455Z",
     "iopub.status.busy": "2020-08-11T22:50:20.810455Z",
     "iopub.status.idle": "2020-08-11T22:50:20.820134Z",
     "shell.execute_reply": "2020-08-11T22:50:20.818445Z",
     "shell.execute_reply.started": "2020-08-11T22:50:20.810455Z"
    }
   },
   "outputs": [],
   "source": [
    "BASELINE_FEATURES = ['smart_5_raw', 'smart_187_raw',\n",
    "                     'smart_188_raw', 'smart_197_raw', 'smart_198_raw']\n",
    "\n",
    "# MODELS = ['baseline',\n",
    "#           'fail_today_xgb',\n",
    "#           'fail_today_or_tomorrow_xgb',\n",
    "#           'fail_this_week_xgb',\n",
    "#           'fail_this_month_xgb',\n",
    "#           'fail_today_xgb_history',\n",
    "#           'fail_today_or_tomorrow_xgb_history',\n",
    "#           'fail_this_week_xgb_history',\n",
    "#           'fail_this_month_xgb_history']\n",
    "MODELS = ['fail_today_forest',\n",
    "          'fail_today_or_tomorrow_forest',\n",
    "          'fail_this_week_forest',\n",
    "          'fail_this_month_forest',\n",
    "          'fail_today_forest_history',\n",
    "          'fail_today_or_tomorrow_forest_history',\n",
    "          'fail_this_week_forest_history',\n",
    "          'fail_this_month_forest_history']\n",
    "\n",
    "HDD_PATH = os.path.join('datasets', 'drive_stats', 'test')\n",
    "\n",
    "# Change these if needed!\n",
    "MODEL_PATH = os.path.join('exploring_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-11T22:50:22.707741Z",
     "iopub.status.busy": "2020-08-11T22:50:22.707741Z",
     "iopub.status.idle": "2020-08-11T22:50:22.721310Z",
     "shell.execute_reply": "2020-08-11T22:50:22.720275Z",
     "shell.execute_reply.started": "2020-08-11T22:50:22.707741Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_test_data(test_location):\n",
    "    \"\"\"\n",
    "    Function to get all test data \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    test_location:\n",
    "        Where test csv's are located\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    concatenated_df : DataFrame\n",
    "        A dataframe containing all of the data from the csvs\n",
    "    \"\"\"\n",
    "    # List the relevant files in the bucket\n",
    "    files = glob.glob(os.path.join(test_location, '*.csv'))\n",
    "    # Read the files, concatenate them, and sort by date\n",
    "    # df_for_each_blob = (pd.read_csv(file) for file in files)\n",
    "    concatenated_df = pd.concat(pd.read_csv(file) for file in files).sort_values('date')\n",
    "    # Some drives failed between Jan 1st and Jan 10th, we remove them here\n",
    "    # (This could also just have been done in BQ ahead of time)\n",
    "    already_removed_drives = concatenated_df[(concatenated_df['failure'] == 1) \n",
    "                                             & (concatenated_df['date'] < '2019-01-10')]['serial_number'].values\n",
    "    concatenated_df = concatenated_df[~concatenated_df['serial_number'].isin(already_removed_drives)]\n",
    "    cols_to_drop = [col for col in concatenated_df.columns if 'normalized' in col]\n",
    "    concatenated_df.drop(columns=cols_to_drop, inplace=True)\n",
    "    concatenated_df\n",
    "    \n",
    "    return concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-11T22:50:23.538630Z",
     "iopub.status.busy": "2020-08-11T22:50:23.538630Z",
     "iopub.status.idle": "2020-08-11T22:50:23.547471Z",
     "shell.execute_reply": "2020-08-11T22:50:23.546441Z",
     "shell.execute_reply.started": "2020-08-11T22:50:23.538630Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(model, model_path):\n",
    "    \"\"\"\n",
    "    Load a saved model so we can use it for predictions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : string\n",
    "        Name of the model\n",
    "        \n",
    "    model_path : string\n",
    "        Path to the saved model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loaded_model : Model\n",
    "        Trained Sklearn/xgboost model used to make predictions\n",
    "    \"\"\"\n",
    "    model_file = os.path.join(model_path, model + '.pickle.dat')\n",
    "    loaded_model = pickle.load(open(model_file, \"rb\"))\n",
    "    \n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-11T23:56:32.894466Z",
     "iopub.status.busy": "2020-08-11T23:56:32.894466Z",
     "iopub.status.idle": "2020-08-11T23:56:32.919177Z",
     "shell.execute_reply": "2020-08-11T23:56:32.918147Z",
     "shell.execute_reply.started": "2020-08-11T23:56:32.894466Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_test(model, test_data, model_path):\n",
    "    \"\"\"\n",
    "    Run a test for a particular model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : string\n",
    "        Name of the model\n",
    "\n",
    "    test_data : DataFrame\n",
    "        Data for which we'd like to run the test\n",
    "\n",
    "    model_path :\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "    \"\"\"\n",
    "    date_list = test_data['date'].unique()\n",
    "\n",
    "    # Load model and figure out which aggregations to do\n",
    "    if model != 'baseline':\n",
    "        loaded_model = get_model(model, model_path)\n",
    "\n",
    "    # Same aggregations we used for training\n",
    "    if 'history' in model:\n",
    "        aggs = ['last', np.mean, np.var, 'min', 'max']\n",
    "    else:\n",
    "        aggs = ['last']\n",
    "\n",
    "    # Keep track of the drives we have \"removed\".\n",
    "    # These represent drives which have failed or that we have\n",
    "    # done maintenance on.\n",
    "    removed_drives = set()\n",
    "    # The two primary metrics we care about\n",
    "    unexpected_breaks = 0\n",
    "    false_alarms = 0\n",
    "\n",
    "    print('{:=^80}'.format(' Evaluating {} '.format(model)))\n",
    "\n",
    "    # Now basically do 10 day sliding windows, where 10\n",
    "    # is the length of our historical window.\n",
    "    for i in range(len(date_list) - 9):\n",
    "        dates = date_list[i:i+10]\n",
    "\n",
    "        # Get 10 days of history for drives that have not been 'removed'\n",
    "        window_df = test_data[(test_data['date'].isin(dates)) & ~(\n",
    "            test_data['serial_number'].isin(removed_drives))]\n",
    "\n",
    "        grouped_df = window_df.groupby('serial_number')\n",
    "        features = grouped_df[[\n",
    "            col for col in window_df.columns if 'smart' in col]]\n",
    "        features = features.agg(aggs)\n",
    "        # Fill nulls\n",
    "        for col in features:\n",
    "            features[col] = features[col].fillna(features[col].mode()[0])\n",
    "\n",
    "        results = grouped_df['failure'].agg(['last'])\n",
    "\n",
    "        if model == 'baseline':\n",
    "            results['prediction'] = np.any(features[BASELINE_FEATURES].values >\n",
    "                                           0, axis=1).astype(int)\n",
    "        else:\n",
    "            results['prediction'] = loaded_model.predict(features)\n",
    "\n",
    "        predicted_failed_drives = set(\n",
    "            results[results['prediction'] == 1].index)\n",
    "        actual_failed_drives = set(results[results['last'] == 1].index)\n",
    "\n",
    "        # Actually failed but not predicted\n",
    "        unexpected_breaks += len(actual_failed_drives\n",
    "                                 - predicted_failed_drives)\n",
    "        # Predicted but didn't fail\n",
    "        false_alarms += len(predicted_failed_drives - actual_failed_drives)\n",
    "        # Both predicted and actually failed are removed\n",
    "        removed_drives.update(\n",
    "            predicted_failed_drives.union(actual_failed_drives))\n",
    "\n",
    "        # There are a lot of false alarms on the first day, keep track of\n",
    "        # this number\n",
    "        if i == 0:\n",
    "            initial_false_alarms = false_alarms\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(\"By {} we have {} unexpected breaks and {} false alarms\"\n",
    "                  .format(dates[-1],\n",
    "                          unexpected_breaks,\n",
    "                          false_alarms))\n",
    "\n",
    "    final_results = {'Unexpected Breaks': unexpected_breaks,\n",
    "                     'False Alarms': false_alarms,\n",
    "                     'Initial False Alarms': initial_false_alarms}\n",
    "\n",
    "    print(final_results)\n",
    "    print('{:-^80}'.format(''))\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-11T23:56:53.593560Z",
     "iopub.status.busy": "2020-08-11T23:56:53.593560Z",
     "iopub.status.idle": "2020-08-11T23:58:26.191944Z",
     "shell.execute_reply": "2020-08-11T23:58:26.191944Z",
     "shell.execute_reply.started": "2020-08-11T23:56:53.593560Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = get_test_data(HDD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-11T23:58:26.191944Z",
     "iopub.status.busy": "2020-08-11T23:58:26.191944Z",
     "iopub.status.idle": "2020-08-12T00:31:48.208240Z",
     "shell.execute_reply": "2020-08-12T00:31:48.208240Z",
     "shell.execute_reply.started": "2020-08-11T23:58:26.191944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Evaluating fail_today_forest =========================\n",
      "By 2019-01-10 we have 1 unexpected breaks and 968 false alarms\n",
      "By 2019-01-30 we have 8 unexpected breaks and 1489 false alarms\n",
      "By 2019-02-19 we have 19 unexpected breaks and 1698 false alarms\n",
      "By 2019-03-11 we have 21 unexpected breaks and 1798 false alarms\n",
      "By 2019-03-31 we have 27 unexpected breaks and 1853 false alarms\n",
      "By 2019-04-20 we have 34 unexpected breaks and 1903 false alarms\n",
      "By 2019-05-10 we have 45 unexpected breaks and 1984 false alarms\n",
      "By 2019-05-30 we have 46 unexpected breaks and 2035 false alarms\n",
      "{'Unexpected Breaks': 46, 'False Alarms': 2037, 'Initial False Alarms': 968}\n",
      "--------------------------------------------------------------------------------\n",
      "=================== Evaluating fail_today_or_tomorrow_forest ===================\n",
      "By 2019-01-10 we have 1 unexpected breaks and 952 false alarms\n",
      "By 2019-01-30 we have 9 unexpected breaks and 1363 false alarms\n",
      "By 2019-02-19 we have 19 unexpected breaks and 1525 false alarms\n",
      "By 2019-03-11 we have 21 unexpected breaks and 1615 false alarms\n",
      "By 2019-03-31 we have 27 unexpected breaks and 1668 false alarms\n",
      "By 2019-04-20 we have 34 unexpected breaks and 1728 false alarms\n",
      "By 2019-05-10 we have 45 unexpected breaks and 1780 false alarms\n",
      "By 2019-05-30 we have 46 unexpected breaks and 1822 false alarms\n",
      "{'Unexpected Breaks': 46, 'False Alarms': 1824, 'Initial False Alarms': 952}\n",
      "--------------------------------------------------------------------------------\n",
      "======================= Evaluating fail_this_week_forest =======================\n",
      "By 2019-01-10 we have 1 unexpected breaks and 675 false alarms\n",
      "By 2019-01-30 we have 9 unexpected breaks and 842 false alarms\n",
      "By 2019-02-19 we have 19 unexpected breaks and 907 false alarms\n",
      "By 2019-03-11 we have 21 unexpected breaks and 962 false alarms\n",
      "By 2019-03-31 we have 27 unexpected breaks and 1018 false alarms\n",
      "By 2019-04-20 we have 33 unexpected breaks and 1053 false alarms\n",
      "By 2019-05-10 we have 44 unexpected breaks and 1084 false alarms\n",
      "By 2019-05-30 we have 47 unexpected breaks and 1109 false alarms\n",
      "{'Unexpected Breaks': 47, 'False Alarms': 1109, 'Initial False Alarms': 675}\n",
      "--------------------------------------------------------------------------------\n",
      "====================== Evaluating fail_this_month_forest =======================\n",
      "By 2019-01-10 we have 0 unexpected breaks and 393 false alarms\n",
      "By 2019-01-30 we have 3 unexpected breaks and 511 false alarms\n",
      "By 2019-02-19 we have 16 unexpected breaks and 560 false alarms\n",
      "By 2019-03-11 we have 20 unexpected breaks and 601 false alarms\n",
      "By 2019-03-31 we have 29 unexpected breaks and 628 false alarms\n",
      "By 2019-04-20 we have 36 unexpected breaks and 656 false alarms\n",
      "By 2019-05-10 we have 49 unexpected breaks and 680 false alarms\n",
      "By 2019-05-30 we have 55 unexpected breaks and 693 false alarms\n",
      "{'Unexpected Breaks': 55, 'False Alarms': 696, 'Initial False Alarms': 393}\n",
      "--------------------------------------------------------------------------------\n",
      "===================== Evaluating fail_today_forest_history =====================\n",
      "By 2019-01-10 we have 1 unexpected breaks and 966 false alarms\n",
      "By 2019-01-30 we have 8 unexpected breaks and 1886 false alarms\n",
      "By 2019-02-19 we have 19 unexpected breaks and 2447 false alarms\n",
      "By 2019-03-11 we have 21 unexpected breaks and 3282 false alarms\n",
      "By 2019-03-31 we have 27 unexpected breaks and 4358 false alarms\n",
      "By 2019-04-20 we have 34 unexpected breaks and 4579 false alarms\n",
      "By 2019-05-10 we have 43 unexpected breaks and 4773 false alarms\n",
      "By 2019-05-30 we have 44 unexpected breaks and 4954 false alarms\n",
      "{'Unexpected Breaks': 44, 'False Alarms': 4964, 'Initial False Alarms': 966}\n",
      "--------------------------------------------------------------------------------\n",
      "=============== Evaluating fail_today_or_tomorrow_forest_history ===============\n",
      "By 2019-01-10 we have 1 unexpected breaks and 817 false alarms\n",
      "By 2019-01-30 we have 9 unexpected breaks and 1318 false alarms\n",
      "By 2019-02-19 we have 19 unexpected breaks and 1772 false alarms\n",
      "By 2019-03-11 we have 21 unexpected breaks and 2302 false alarms\n",
      "By 2019-03-31 we have 27 unexpected breaks and 2833 false alarms\n",
      "By 2019-04-20 we have 34 unexpected breaks and 3075 false alarms\n",
      "By 2019-05-10 we have 45 unexpected breaks and 3174 false alarms\n",
      "By 2019-05-30 we have 46 unexpected breaks and 3238 false alarms\n",
      "{'Unexpected Breaks': 46, 'False Alarms': 3245, 'Initial False Alarms': 817}\n",
      "--------------------------------------------------------------------------------\n",
      "=================== Evaluating fail_this_week_forest_history ===================\n",
      "By 2019-01-10 we have 1 unexpected breaks and 651 false alarms\n",
      "By 2019-01-30 we have 9 unexpected breaks and 762 false alarms\n",
      "By 2019-02-19 we have 20 unexpected breaks and 838 false alarms\n",
      "By 2019-03-11 we have 22 unexpected breaks and 936 false alarms\n",
      "By 2019-03-31 we have 28 unexpected breaks and 999 false alarms\n",
      "By 2019-04-20 we have 35 unexpected breaks and 1029 false alarms\n",
      "By 2019-05-10 we have 46 unexpected breaks and 1056 false alarms\n",
      "By 2019-05-30 we have 47 unexpected breaks and 1081 false alarms\n",
      "{'Unexpected Breaks': 47, 'False Alarms': 1081, 'Initial False Alarms': 651}\n",
      "--------------------------------------------------------------------------------\n",
      "================== Evaluating fail_this_month_forest_history ===================\n",
      "By 2019-01-10 we have 0 unexpected breaks and 402 false alarms\n",
      "By 2019-01-30 we have 3 unexpected breaks and 540 false alarms\n",
      "By 2019-02-19 we have 16 unexpected breaks and 606 false alarms\n",
      "By 2019-03-11 we have 19 unexpected breaks and 679 false alarms\n",
      "By 2019-03-31 we have 27 unexpected breaks and 716 false alarms\n",
      "By 2019-04-20 we have 34 unexpected breaks and 747 false alarms\n",
      "By 2019-05-10 we have 48 unexpected breaks and 778 false alarms\n",
      "By 2019-05-30 we have 54 unexpected breaks and 809 false alarms\n",
      "{'Unexpected Breaks': 54, 'False Alarms': 809, 'Initial False Alarms': 402}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Location of saved models on disk\n",
    "model_path = MODEL_PATH\n",
    "\n",
    "results = {}\n",
    "for model in MODELS:\n",
    "    results[model] = run_test(model, test_data, model_path)\n",
    "\n",
    "results_frame = pd.DataFrame(results).T\n",
    "results_frame.to_csv('test_results_forest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
